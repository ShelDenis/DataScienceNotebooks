# -*- coding: utf-8 -*-
"""Повторение перед Демоэкзаменом. Регрессия.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14iGdXdhOoqtDmSHQmtxkQMehftAHmGVj
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

"""Тип задачи - регрессия. Целевая переменная - `song_popularity`"""

!python --version

seed = 42
np.random.seed(seed)

data = pd.read_csv('song_data.csv')
data

data.info()

from sklearn.model_selection import train_test_split

data = data.drop(['song_name'], axis=1)

y = data['song_popularity']
X = data.drop(['song_popularity'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)
X_train

from matplotlib.axes._axes import Axes


def draw_sns(df, visualization_function, figsize=(20, 20), **kwargs):
    n_col = 3
    n = len(df.columns)
    n_row = int(np.ceil(n / n_col))

    _, ax = plt.subplots(n_row, n_col, figsize=figsize)

    columns = df.columns

    for i in range(n_row):
        for j in range(n_col):
            k = i * n_col + j
            if k == n:
                break
            column = columns[k]
            x = df[column]
            visualization_function(x, ax=ax[i, j], **kwargs)

draw_sns(X_train, visualization_function=sns.histplot, log=True)

draw_sns(X_train, sns.boxplot, figsize=(20, 25))

def calc_bounds(x):
    q1, q3 = np.percentile(x, [25, 75])
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return lower_bound, upper_bound

def check_feature_outliers_iqr(x: pd.Series):
    lower_bound, upper_bound = calc_bounds(x)
    return ((x < lower_bound) | (x > upper_bound)).values

def check_dataset_outliers(df, outliers_method, **kwargs):
    outliers_flags = [
        outliers_method(df[column], **kwargs) for column in df.columns
    ]
    return np.any(np.array(outliers_flags), axis=0)

outliers_objects_iqr = X_train.loc[check_dataset_outliers(X_train[['song_duration_ms', 'energy', 'loudness']], check_feature_outliers_iqr)]
outliers_objects_iqr

X_train_pure = X_train.drop(outliers_objects_iqr.index)
y_train_pure = y_train.drop(outliers_objects_iqr.index)
y_train_pure

draw_sns(X_train_pure, visualization_function=sns.histplot, log=True)

def draw_anomaly(X_2d, anomaly_mask):
    plt.scatter(X_2d[:, 0], X_2d[:, 1])
    plt.scatter(X_2d[anomaly_mask, 0], X_2d[anomaly_mask, 1], c="red", marker='x')

def draw_anomaly_pie(anomaly_mask):
    _, counts = np.unique(anomaly_mask, return_counts=True)

    plt.pie(counts, labels=counts, autopct='%1.0f%%');
    plt.legend([
        'Нормальные объекты', 'Аномалии'
    ], loc='lower right');

from umap.umap_ import UMAP

X_2d = UMAP().fit_transform(X_train_pure)

!pip install pyod

from pyod.models.iforest import IForest


isf = IForest().fit(X_train_pure)
isf_anomaly = isf.predict(X_train_pure)

draw_anomaly_pie(isf_anomaly)

draw_anomaly(X_2d, isf_anomaly.astype(bool))

from pyod.models.ocsvm import OCSVM


ocsvm = OCSVM().fit(X_train_pure)
ocsvm_anomaly = ocsvm.predict(X_train_pure)

draw_anomaly_pie(ocsvm_anomaly)

draw_anomaly(X_2d, ocsvm_anomaly.astype(bool))

from pyod.models.lof import LOF

lf = LOF().fit(X_train_pure)
lf_anomaly = lf.predict(X_train_pure)

draw_anomaly_pie(lf_anomaly)

draw_anomaly(X_2d, lf_anomaly.astype(bool))

corr = X_train_pure.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.show()

corr_with_target = X_train_pure.corrwith(y_train_pure)
heatmap = pd.DataFrame(corr_with_target).T

plt.figure(figsize=(10, 5))
sns.heatmap(heatmap, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, fmt='.2f')
plt.title('Корреляции с целевым признаком')
plt.ylabel('Популярность')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

features = X_train_pure.columns
target = y_train_pure

n_features = len(features)
n_cols = 2
n_rows = (n_features + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))
axes = axes.flatten()

for idx, feature in enumerate(features):
    axes[idx].scatter(X_train_pure[feature], y_train_pure, alpha=0.6)
    axes[idx].set_xlabel(feature)
    axes[idx].set_ylabel('Популярность')

plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_pure)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_pure.columns, index=X_train_pure.index)
X_test_scaled = scaler.transform(X_test)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
X_train_scaled

dtr_params = {
    'max_depth': ('int', [3, 7]),
    'criterion': ('cat', ['squared_error', 'absolute_error', 'poisson']),
    'min_samples_split': ('int', [2, 10])
}

rf_params = {
    'n_estimators': ('cat', [50, 100, 300]),
    'max_depth': ('int', [3, 7]),
    'min_samples_split': ('int', [2, 10])
}

gbr_params = {
    'n_estimators': ('cat', [50, 100, 300]),
    'max_depth': ('int', [3, 7]),
    'min_samples_split': ('int', [2, 10]),
    'loss': ('cat', ['squared_error', 'absolute_error', 'huber', 'quantile'])
}

!pip install optuna

import optuna
from sklearn.model_selection import cross_val_score


def gen_objective(estimator_class, grid, X_train, y_train, **kwargs):
    def objective(trial):
        params = {}
        for k, v in grid.items():
            if v[0] == 'cat':
                params[k] = trial.suggest_categorical(k, v[1])
            elif v[0] == 'int':
                params[k] = trial.suggest_int(k, *v[1])
            elif v[0] == 'float':
                params[k] = trial.suggest_float(k, *v[1], log=True)

        regressor = estimator_class(**params)

        mse_scores = cross_val_score(regressor, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
        return -mse_scores.mean()

    return objective

def get_best_params(X, y, model, params):
    my_objective = gen_objective(model, params, X, y)
    study = optuna.create_study(direction="minimize")
    study.optimize(my_objective, n_trials=10)
    return study.best_params

from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score


def get_metrics(model_name, y_test, y_pred):
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    return model_name, {'MAE': mae, 'MSE': mse, 'MAPE': mape, 'R2': r2}

metdct = {}

from sklearn.tree import DecisionTreeRegressor

new_feat_dtr = DecisionTreeRegressor(**get_best_params(X_train_pure, y_train_pure, DecisionTreeRegressor, dtr_params))
new_feat_dtr.fit(X_train_pure, y_train_pure)
y_pr = new_feat_dtr.predict(X_test)
name, vals = mtrs = get_metrics('new_feat - DTR', y_test, y_pr)
metdct[name] = vals
mtrs

from sklearn.ensemble import RandomForestRegressor

new_feat_rf = RandomForestRegressor(**get_best_params(X_train_pure, y_train_pure, RandomForestRegressor, rf_params))
new_feat_rf.fit(X_train_pure, y_train_pure)
y_pr = new_feat_rf.predict(X_test)
name, vals = mtrs = get_metrics('new_feat - RF', y_test, y_pr)
metdct[name] = vals
mtrs

from sklearn.ensemble import GradientBoostingRegressor

new_feat_gbr = GradientBoostingRegressor(**get_best_params(X_train_pure, y_train_pure, GradientBoostingRegressor, gbr_params))
new_feat_gbr.fit(X_train_pure, y_train_pure)
y_pr = new_feat_gbr.predict(X_test)
name, vals = mtrs = get_metrics('new_feat - GBR', y_test, y_pr)
metdct[name] = vals
mtrs

scaled_dtr = DecisionTreeRegressor(**get_best_params(X_train_scaled, y_train_pure, DecisionTreeRegressor, dtr_params))
scaled_dtr.fit(X_train_scaled, y_train_pure)
y_pr = scaled_dtr.predict(X_test_scaled)
name, vals = mtrs = get_metrics('scaled - DTR', y_test, y_pr)
metdct[name] = vals
mtrs

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train_pure, y_train_pure)
y_pred = lr.predict(X_test)
name, vals = get_metrics('simple - LR', y_test, y_pred)
vals