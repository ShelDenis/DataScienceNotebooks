{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e9229e",
   "metadata": {},
   "source": [
    "# Классификация текстов с применением RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d36d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>public_petition_text</th>\n",
       "      <th>reason_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3168490</td>\n",
       "      <td>снег на дороге</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3219678</td>\n",
       "      <td>очистить кабельный киоск от рекламы</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2963920</td>\n",
       "      <td>Просим убрать все деревья и кустарники, которы...</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3374910</td>\n",
       "      <td>Неудовлетворительное состояние парадной - надп...</td>\n",
       "      <td>Содержание МКД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3336285</td>\n",
       "      <td>Граффити</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               public_petition_text  reason_category\n",
       "0  3168490                                     снег на дороге  Благоустройство\n",
       "1  3219678                очистить кабельный киоск от рекламы  Благоустройство\n",
       "2  2963920  Просим убрать все деревья и кустарники, которы...  Благоустройство\n",
       "3  3374910  Неудовлетворительное состояние парадной - надп...   Содержание МКД\n",
       "4  3336285                                           Граффити  Благоустройство"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('D:/PythonWork/OldZmiy/NLPWork/data/Petitions.csv')[:10000]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db4c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sheld\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sheld\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sheld\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>public_petition_text</th>\n",
       "      <th>reason_category</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3168490</td>\n",
       "      <td>снег на дороге</td>\n",
       "      <td>Благоустройство</td>\n",
       "      <td>снег дорог</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3219678</td>\n",
       "      <td>очистить кабельный киоск от рекламы</td>\n",
       "      <td>Благоустройство</td>\n",
       "      <td>очист кабельн киоск реклам</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2963920</td>\n",
       "      <td>Просим убрать все деревья и кустарники, которы...</td>\n",
       "      <td>Благоустройство</td>\n",
       "      <td>прос убра дерев кустарник котор вышл предел га...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3374910</td>\n",
       "      <td>Неудовлетворительное состояние парадной - надп...</td>\n",
       "      <td>Содержание МКД</td>\n",
       "      <td>неудовлетворительн состоян парадн надпис двер ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3336285</td>\n",
       "      <td>Граффити</td>\n",
       "      <td>Благоустройство</td>\n",
       "      <td>граффит</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               public_petition_text  \\\n",
       "0  3168490                                     снег на дороге   \n",
       "1  3219678                очистить кабельный киоск от рекламы   \n",
       "2  2963920  Просим убрать все деревья и кустарники, которы...   \n",
       "3  3374910  Неудовлетворительное состояние парадной - надп...   \n",
       "4  3336285                                           Граффити   \n",
       "\n",
       "   reason_category                                  preprocessed_text  \n",
       "0  Благоустройство                                         снег дорог  \n",
       "1  Благоустройство                         очист кабельн киоск реклам  \n",
       "2  Благоустройство  прос убра дерев кустарник котор вышл предел га...  \n",
       "3   Содержание МКД  неудовлетворительн состоян парадн надпис двер ...  \n",
       "4  Благоустройство                                            граффит  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    stems = [stemmer.stem(w) for w in filtered_tokens]\n",
    "    return ' '.join(stems)\n",
    "\n",
    "data['preprocessed_text'] = data['public_petition_text'].apply(preprocess)\n",
    "\n",
    "data.to_csv('preprocessed_dataset.csv', index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d2c109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Имена признаков: ['covid' 'dns' 'err' ... 'ясельн' 'яхтен' 'ящик']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(data['preprocessed_text'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"Имена признаков:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba71d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           id                               public_petition_text  \\\n",
       " 0     3168490                                     снег на дороге   \n",
       " 1     3219678                очистить кабельный киоск от рекламы   \n",
       " 2     2963920  Просим убрать все деревья и кустарники, которы...   \n",
       " 3     3374910  Неудовлетворительное состояние парадной - надп...   \n",
       " 4     3336285                                           Граффити   \n",
       " ...       ...                                                ...   \n",
       " 9995  3236509                                      очистите о  о   \n",
       " 9996  3213091  На фасаде дома, незаконно и без разрешающих до...   \n",
       " 9997  3242261  Товарный переулок. Мусор. В администрацию Цент...   \n",
       " 9998  3311922  В проезжей части просел канализационный люк.\\n...   \n",
       " 9999  3049014                                  Надписи на будке.   \n",
       " \n",
       "       reason_category                                  preprocessed_text  \n",
       " 0                   1                                         снег дорог  \n",
       " 1                   1                         очист кабельн киоск реклам  \n",
       " 2                   1  прос убра дерев кустарник котор вышл предел га...  \n",
       " 3                   2  неудовлетворительн состоян парадн надпис двер ...  \n",
       " 4                   1                                            граффит  \n",
       " ...               ...                                                ...  \n",
       " 9995                1                                              очист  \n",
       " 9996                6  фасад дом незакон разреша документ установл на...  \n",
       " 9997                1  товарн переулок мусор администрац центральн район  \n",
       " 9998                7  проезж част просел канализацион люк треб произ...  \n",
       " 9999                1                                        надпис будк  \n",
       " \n",
       " [10000 rows x 4 columns],\n",
       " {'Благоустройство': 1,\n",
       "  'Содержание МКД': 2,\n",
       "  'Незаконная информационная и (или) рекламная конструкция': 3,\n",
       "  'Фасад': 4,\n",
       "  'Водоснабжение': 5,\n",
       "  'Нарушение правил пользования общим имуществом': 6,\n",
       "  'Повреждения или неисправность элементов уличной инфраструктуры': 7,\n",
       "  'Кровля': 8,\n",
       "  'Состояние рекламных или информационных конструкций': 9,\n",
       "  'Нарушение порядка пользования общим имуществом': 10,\n",
       "  'Подвалы': 11,\n",
       "  'Водоотведение': 12,\n",
       "  'Санитарное состояние': 13,\n",
       "  'Центральное отопление': 14,\n",
       "  'Незаконная реализация товаров с торгового оборудования (прилавок, ящик, с земли)': 15})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_mapping = {}\n",
    "\n",
    "unique_values = data['reason_category'].unique().tolist() \n",
    "mapping_dict = {val: idx+1 for idx, val in enumerate(unique_values)}  \n",
    "categories_mapping['reason_category'] = mapping_dict\n",
    "    \n",
    "data['reason_category'] = data['reason_category'].map(mapping_dict)\n",
    "\n",
    "data, mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11c2a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6252     9\n",
       "4684     2\n",
       "1731     5\n",
       "4742     2\n",
       "4521     1\n",
       "        ..\n",
       "6412     1\n",
       "8285     1\n",
       "7853    14\n",
       "1095     1\n",
       "6929     2\n",
       "Name: reason_category, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, data['reason_category'], test_size=0.2, random_state=42)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7fc89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.todense().astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.todense().astype(np.float32))\n",
    "y_test_tensor = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eee0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37794bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c67530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(f\"Точность модели: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9bb08",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3168df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb2c128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        last_hidden_state = rnn_out[:, -1, :]\n",
    "        return self.fc(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ac2263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Средняя потеря - 2.0089\n",
      "Эпоха 2: Средняя потеря - 1.0520\n",
      "Эпоха 3: Средняя потеря - 0.7348\n",
      "Эпоха 4: Средняя потеря - 0.5465\n",
      "Эпоха 5: Средняя потеря - 0.4237\n",
      "Эпоха 6: Средняя потеря - 0.3408\n",
      "Эпоха 7: Средняя потеря - 0.2796\n",
      "Эпоха 8: Средняя потеря - 0.2333\n",
      "Эпоха 9: Средняя потеря - 0.1985\n",
      "Эпоха 10: Средняя потеря - 0.1730\n",
      "Эпоха 11: Средняя потеря - 0.1504\n",
      "Эпоха 12: Средняя потеря - 0.1345\n",
      "Эпоха 13: Средняя потеря - 0.1210\n",
      "Эпоха 14: Средняя потеря - 0.1107\n",
      "Эпоха 15: Средняя потеря - 0.1029\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 7436 \n",
    "hidden_size = 128\n",
    "output_size = 16 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "model = RNN(embed_dim, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        \n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Эпоха {epoch+1}: Средняя потеря - {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e0de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.95      0.94      1221\n",
      "           2       0.82      0.91      0.86       453\n",
      "           3       0.96      0.91      0.93        55\n",
      "           4       0.74      0.53      0.62        49\n",
      "           5       0.80      0.89      0.84        18\n",
      "           6       0.98      0.86      0.92        65\n",
      "           7       0.92      0.90      0.91        39\n",
      "           8       0.88      0.73      0.80        30\n",
      "           9       0.78      0.30      0.44        23\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       1.00      0.60      0.75         5\n",
      "          12       0.88      0.64      0.74        11\n",
      "          13       0.88      0.47      0.61        15\n",
      "          14       1.00      0.50      0.67         8\n",
      "          15       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.84      0.66      0.72      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PythonWork\\OldZmiy\\NLPWork\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\PythonWork\\OldZmiy\\NLPWork\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\PythonWork\\OldZmiy\\NLPWork\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor.unsqueeze(1))\n",
    "    probs = outputs.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "y_pred = predictions.cpu().numpy()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e214d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        return self.fc(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e92bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Средняя потеря - 2.4527\n",
      "Эпоха 2: Средняя потеря - 1.3808\n",
      "Эпоха 3: Средняя потеря - 0.9930\n",
      "Эпоха 4: Средняя потеря - 0.7510\n",
      "Эпоха 5: Средняя потеря - 0.5927\n",
      "Эпоха 6: Средняя потеря - 0.4818\n",
      "Эпоха 7: Средняя потеря - 0.3945\n",
      "Эпоха 8: Средняя потеря - 0.3284\n",
      "Эпоха 9: Средняя потеря - 0.2770\n",
      "Эпоха 10: Средняя потеря - 0.2391\n",
      "Эпоха 11: Средняя потеря - 0.2075\n",
      "Эпоха 12: Средняя потеря - 0.1808\n",
      "Эпоха 13: Средняя потеря - 0.1601\n",
      "Эпоха 14: Средняя потеря - 0.1438\n",
      "Эпоха 15: Средняя потеря - 0.1292\n",
      "Эпоха 16: Средняя потеря - 0.1172\n",
      "Эпоха 17: Средняя потеря - 0.1077\n",
      "Эпоха 18: Средняя потеря - 0.1003\n",
      "Эпоха 19: Средняя потеря - 0.0946\n",
      "Эпоха 20: Средняя потеря - 0.0884\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 7436 \n",
    "hidden_size = 128\n",
    "output_size = 16 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "model2 = MyLSTM(embed_dim, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        \n",
    "        outputs = model2(inputs.unsqueeze(1))\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Эпоха {epoch+1}: Средняя потеря - {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12385d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.95      0.94      1221\n",
      "           2       0.81      0.89      0.85       453\n",
      "           3       0.94      0.89      0.92        55\n",
      "           4       0.75      0.55      0.64        49\n",
      "           5       0.80      0.89      0.84        18\n",
      "           6       0.98      0.86      0.92        65\n",
      "           7       0.92      0.92      0.92        39\n",
      "           8       0.88      0.70      0.78        30\n",
      "           9       0.75      0.26      0.39        23\n",
      "          10       0.50      0.40      0.44         5\n",
      "          11       1.00      0.60      0.75         5\n",
      "          12       0.88      0.64      0.74        11\n",
      "          13       0.86      0.40      0.55        15\n",
      "          14       1.00      0.50      0.67         8\n",
      "          15       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.87      0.67      0.74      2000\n",
      "weighted avg       0.90      0.90      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model2(X_test_tensor.unsqueeze(1))\n",
    "    probs = outputs.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "y_pred = predictions.cpu().numpy()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2a47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_hidden_state = gru_out[:, -1, :]\n",
    "        return self.fc(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab357b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Средняя потеря - 1.6287\n",
      "Эпоха 2: Средняя потеря - 0.6458\n",
      "Эпоха 3: Средняя потеря - 0.3783\n",
      "Эпоха 4: Средняя потеря - 0.2484\n",
      "Эпоха 5: Средняя потеря - 0.1756\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 7436 \n",
    "hidden_size = 1024\n",
    "output_size = 16 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "model3 = MyGRU(embed_dim, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        \n",
    "        outputs = model3(inputs.unsqueeze(1))\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Эпоха {epoch+1}: Средняя потеря - {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e952a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.95      0.94      1221\n",
      "           2       0.83      0.91      0.87       453\n",
      "           3       0.93      0.91      0.92        55\n",
      "           4       0.80      0.57      0.67        49\n",
      "           5       0.80      0.89      0.84        18\n",
      "           6       0.98      0.86      0.92        65\n",
      "           7       0.92      0.87      0.89        39\n",
      "           8       0.88      0.77      0.82        30\n",
      "           9       0.75      0.26      0.39        23\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       1.00      0.40      0.57         5\n",
      "          12       1.00      0.64      0.78        11\n",
      "          13       0.83      0.33      0.48        15\n",
      "          14       1.00      0.25      0.40         8\n",
      "          15       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.84      0.60      0.67      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PythonWork\\OldZmiy\\NLPWork\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\PythonWork\\OldZmiy\\NLPWork\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\PythonWork\\OldZmiy\\NLPWork\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "model3.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model3(X_test_tensor.unsqueeze(1))\n",
    "    probs = outputs.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "y_pred = predictions.cpu().numpy()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc357d",
   "metadata": {},
   "source": [
    "RNN и LSTM (accuracy = 0.9) отработали лучше, чем GRU (accuracy = 0.88) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cb8c0",
   "metadata": {},
   "source": [
    "## Модели с вниманием"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698efd9b",
   "metadata": {},
   "source": [
    "### Слой внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a72f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "    \n",
    "    def forward(self, out, h):\n",
    "        # e_i (скалярное произведение скрытых состояний с итоговым скрытым состоянием)\n",
    "        es = torch.bmm(out, h.unsqueeze(2)) \n",
    "        es = es.squeeze(2)\n",
    "        \n",
    "        # a_i (коэффициенты внимания)\n",
    "        alphas = F.softmax(es, dim=1)\n",
    "        \n",
    "        # a_1 * h_1 + ... + a_n * h_n (применяем коэф-ты внимания к скрытым состояниям)\n",
    "        alphas = alphas.unsqueeze(1)  \n",
    "        a_h_sum = torch.bmm(alphas, out)  \n",
    "        a_h_sum = a_h_sum.squeeze(1)\n",
    "        \n",
    "        # Конкатенируем финальное состояние с полученной суммой\n",
    "        c = torch.cat([h, a_h_sum], dim=1) \n",
    "        \n",
    "        return c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b12945",
   "metadata": {},
   "source": [
    "### RNN с вниманием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ddb3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNWithAttention, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size) # (из-за конкатенации двойной размер)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, h = self.rnn(x)\n",
    "        h = h.squeeze(0) \n",
    "        c = self.attention(rnn_out, h)  \n",
    "        output = self.fc(c) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7075927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Средняя потеря - 1.2065\n",
      "Эпоха 2: Средняя потеря - 0.4205\n",
      "Эпоха 3: Средняя потеря - 0.2247\n",
      "Эпоха 4: Средняя потеря - 0.1433\n",
      "Эпоха 5: Средняя потеря - 0.1055\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 7436 \n",
    "hidden_size = 1024\n",
    "output_size = 16 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "att_rnn = RNNWithAttention(embed_dim, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(att_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        \n",
    "        outputs = att_rnn(inputs.unsqueeze(1))\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Эпоха {epoch+1}: Средняя потеря - {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13075bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.95      0.94      1221\n",
      "           2       0.82      0.91      0.86       453\n",
      "           3       0.96      0.91      0.93        55\n",
      "           4       0.76      0.53      0.63        49\n",
      "           5       0.80      0.89      0.84        18\n",
      "           6       1.00      0.88      0.93        65\n",
      "           7       0.92      0.92      0.92        39\n",
      "           8       0.96      0.73      0.83        30\n",
      "           9       0.80      0.35      0.48        23\n",
      "          10       0.50      0.20      0.29         5\n",
      "          11       1.00      0.60      0.75         5\n",
      "          12       0.88      0.64      0.74        11\n",
      "          13       0.88      0.47      0.61        15\n",
      "          14       1.00      0.50      0.67         8\n",
      "          15       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.88      0.68      0.75      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "att_rnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = att_rnn(X_test_tensor.unsqueeze(1))\n",
    "    probs = outputs.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "y_pred = predictions.cpu().numpy()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba6647",
   "metadata": {},
   "source": [
    "По сравнению с обычной RNN, все классы предсказываются и довольно неплохо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fb5ee",
   "metadata": {},
   "source": [
    "### LSTM с вниманием "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0124a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size) # (из-за конкатенации двойной размер)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h, c) = self.lstm(x)  \n",
    "        h = h.squeeze(0)  \n",
    "        context = self.attention(lstm_out, h)  \n",
    "        output = self.fc(context)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4af94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Средняя потеря - 2.2785\n",
      "Эпоха 2: Средняя потеря - 1.2176\n",
      "Эпоха 3: Средняя потеря - 0.8649\n",
      "Эпоха 4: Средняя потеря - 0.6174\n",
      "Эпоха 5: Средняя потеря - 0.4746\n",
      "Эпоха 6: Средняя потеря - 0.3710\n",
      "Эпоха 7: Средняя потеря - 0.2991\n",
      "Эпоха 8: Средняя потеря - 0.2465\n",
      "Эпоха 9: Средняя потеря - 0.2081\n",
      "Эпоха 10: Средняя потеря - 0.1778\n",
      "Эпоха 11: Средняя потеря - 0.1526\n",
      "Эпоха 12: Средняя потеря - 0.1326\n",
      "Эпоха 13: Средняя потеря - 0.1170\n",
      "Эпоха 14: Средняя потеря - 0.1046\n",
      "Эпоха 15: Средняя потеря - 0.0955\n",
      "Эпоха 16: Средняя потеря - 0.0888\n",
      "Эпоха 17: Средняя потеря - 0.0830\n",
      "Эпоха 18: Средняя потеря - 0.0779\n",
      "Эпоха 19: Средняя потеря - 0.0747\n",
      "Эпоха 20: Средняя потеря - 0.0709\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 7436 \n",
    "hidden_size = 128\n",
    "output_size = 16 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "att_lstm = LSTMWithAttention(embed_dim, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(att_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        \n",
    "        outputs = att_lstm(inputs.unsqueeze(1))\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Эпоха {epoch+1}: Средняя потеря - {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e65bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.94      0.94      1221\n",
      "           2       0.81      0.91      0.86       453\n",
      "           3       0.92      0.89      0.91        55\n",
      "           4       0.77      0.55      0.64        49\n",
      "           5       0.80      0.89      0.84        18\n",
      "           6       0.98      0.88      0.93        65\n",
      "           7       0.92      0.92      0.92        39\n",
      "           8       0.88      0.70      0.78        30\n",
      "           9       0.67      0.17      0.28        23\n",
      "          10       0.60      0.60      0.60         5\n",
      "          11       1.00      0.60      0.75         5\n",
      "          12       0.78      0.64      0.70        11\n",
      "          13       0.78      0.47      0.58        15\n",
      "          14       1.00      0.50      0.67         8\n",
      "          15       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.86      0.69      0.75      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "att_lstm.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = att_lstm(X_test_tensor.unsqueeze(1))\n",
    "    probs = outputs.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "y_pred = predictions.cpu().numpy()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1bcde",
   "metadata": {},
   "source": [
    "По сравнению с обычной LSTM, метрики стали выше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06155c",
   "metadata": {},
   "source": [
    "### GRU с вниманием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a2dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUWithAttention, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, h = self.gru(x)\n",
    "        h = h.squeeze(0) \n",
    "        context = self.attention(gru_out, h) \n",
    "        output = self.fc(context)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "289f5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Средняя потеря - 1.4603\n",
      "Эпоха 2: Средняя потеря - 0.5313\n",
      "Эпоха 3: Средняя потеря - 0.2975\n",
      "Эпоха 4: Средняя потеря - 0.1916\n",
      "Эпоха 5: Средняя потеря - 0.1329\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 7436 \n",
    "hidden_size = 1024\n",
    "output_size = 16 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "att_GRU = GRUWithAttention(embed_dim, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(att_GRU.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        \n",
    "        outputs = att_GRU(inputs.unsqueeze(1))\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    " \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Эпоха {epoch+1}: Средняя потеря - {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80823aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.95      0.94      1221\n",
      "           2       0.83      0.89      0.86       453\n",
      "           3       0.96      0.91      0.93        55\n",
      "           4       0.72      0.57      0.64        49\n",
      "           5       0.80      0.89      0.84        18\n",
      "           6       1.00      0.86      0.93        65\n",
      "           7       0.92      0.90      0.91        39\n",
      "           8       0.82      0.77      0.79        30\n",
      "           9       0.82      0.39      0.53        23\n",
      "          10       0.67      0.40      0.50         5\n",
      "          11       1.00      0.60      0.75         5\n",
      "          12       0.88      0.64      0.74        11\n",
      "          13       0.88      0.47      0.61        15\n",
      "          14       1.00      0.50      0.67         8\n",
      "          15       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.88      0.69      0.76      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "att_GRU.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = att_GRU(X_test_tensor.unsqueeze(1))\n",
    "    probs = outputs.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "\n",
    "y_true = y_test_tensor.cpu().numpy()\n",
    "y_pred = predictions.cpu().numpy()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de095af4",
   "metadata": {},
   "source": [
    "По сравнению с обычной GRU, метрики стали выше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
